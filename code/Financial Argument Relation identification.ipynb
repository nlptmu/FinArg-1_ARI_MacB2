{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#read train file\n",
    "df_train = pd.read_csv(f'simple_CH_train.csv')\n",
    "df_test = pd.read_csv(f'simple_CH_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def incremental_concat(df):\n",
    "    df_copy = df.copy()\n",
    "    df = pd.concat([df, df_copy], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "df_train = incremental_concat(df_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# organising materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only text, label\n",
    "data_label = df_train[\"label\"].values\n",
    "y_train = df_train['label'].values\n",
    "df_train = df_train[['text1','text2','combined_text']]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 280\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertTokenizerFast,AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "# Load the BERT tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-macbert-large\")\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent1, sent2, sent3 in zip(data['text1'].astype(str), data['text2'].astype(str), data['combined_text'].astype(str)):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent1, sent2 + \" [SEP] \" + sent3,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation_strategy='longest_first',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'].squeeze())\n",
    "        attention_masks.append(encoded_dict['attention_mask'].squeeze())\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'].squeeze())\n",
    "\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    attention_masks = torch.stack(attention_masks, dim=0)\n",
    "    token_type_ids = torch.stack(token_type_ids, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids\n",
    "\n",
    "train_inputs, train_masks,train_token_type_ids = preprocessing_for_bert(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_token_type_ids,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import BertModel, AutoModel,BertForSequenceClassification, AutoModelForTokenClassification, BertConfig\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_model, freeze_bert=False):#æ”¹\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_model.config.hidden_size, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids = token_type_ids\n",
    "                           )\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "H = 24  # Example value for the hidden size\n",
    "D_out = 3\n",
    "bert_model = BertModel.from_pretrained(\"hfl/chinese-macbert-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "Epochs = 4\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(bert_model)\n",
    "    # bert_classifier = BertClassifier()\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(bert_classifier.parameters(), lr=LEARNING_RATE)\n",
    "    # optimizer = torch.optim.AdamW(bert_classifier.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot train loss\n",
    "    ax.plot(train_loss, label='Train Loss')\n",
    "\n",
    "    # Plot val loss\n",
    "    ax.plot(val_loss, label='Val Loss')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss() # No adjust weight\n",
    "from sklearn.metrics import f1_score\n",
    "# loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(class_weights, dtype=torch.float)) # Adjust weight\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=Epochs, evaluation=False, save_best_model = True):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_macro_f1 = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Macro-F1':^9.2} | {'Elapsed':^9}\")\n",
    "\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_token_type_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask, b_token_type_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9.2} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(\"-\"*80)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, macro_f1 = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if save_best_model:\n",
    "                    torch.save(model.state_dict(), f\"./final_github/best{epoch_i}_model.pt\")\n",
    "            \n",
    "            if macro_f1 > best_macro_f1:\n",
    "                best_macro_f1 = macro_f1\n",
    "                if save_best_model:\n",
    "                    torch.save(model.state_dict(), f\"./final_github/best{epoch_i}_model.pt\")\n",
    "                    \n",
    "            # Append train and val losses to the lists\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {avg_train_loss:^12f} | {val_loss:^10f} | {macro_f1:^9.2f} | {time_elapsed:^9f}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    return best_val_loss, best_macro_f1\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_loss = []\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    val_accuracy = []\n",
    "    macro_f1 = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_token_type_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask, b_token_type_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        val_predictions.extend(preds.cpu().numpy())\n",
    "        val_labels.extend(b_labels.cpu().numpy())\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "        # Compute the macro F1 score\n",
    "        macro_f1_score = f1_score(val_labels, val_predictions, average='macro')\n",
    "        macro_f1.append(macro_f1_score)\n",
    "\n",
    "    # Compute the average loss over the validation set\n",
    "    val_loss = np.mean(val_loss)\n",
    "    macro_f1 = np.mean(macro_f1)\n",
    "\n",
    "    return val_loss, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "batch_size = 16\n",
    "Epoch = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_macro_f1 = 0.0\n",
    "best_model = None\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, train_labels)):\n",
    "    bert_model = BertModel.from_pretrained(\"hfl/chinese-macbert-large\")\n",
    "    print(f\"Fold {fold}\")\n",
    "    fold_performance = []\n",
    "    fold_metrics = []\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs[train_idx], train_masks[train_idx], train_token_type_ids[train_idx],\n",
    "                               train_labels[train_idx])\n",
    "    train_sampler = RandomSampler(train_data)  # No adjust weight\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(train_inputs[val_idx], train_masks[val_idx], train_token_type_ids[val_idx],\n",
    "                             train_labels[val_idx])\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    %time\n",
    "    set_seed(42)  # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=Epoch)\n",
    "    val_loss, macro_f1= train(bert_classifier, train_dataloader, val_dataloader, epochs=Epoch, evaluation=True, save_best_model = True)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = bert_classifier.state_dict()\n",
    "\n",
    "    # Check if the current validation accuracy is better than the best validation accuracy\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "\n",
    "# Save the best model\n",
    "if best_model is not None:\n",
    "    torch.save(best_model, f\"./final_github/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"            \n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_token_type_ids = tuple(t.to(device) for t in batch)[:3]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask, b_token_type_ids)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "                                    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_df = df_test[['text1','text2','combined_text']]\n",
    "test_label = df_test[\"label\"].values\n",
    "y_test = test_label\n",
    "test_inputs, test_masks,test_token_type_ids = preprocessing_for_bert(test_text_df)\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_token_type_ids)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "all_prob = []\n",
    "all_answer = []\n",
    "\n",
    "label_folder_name = './final_github'\n",
    "\n",
    "if not os.path.exists(label_folder_name):\n",
    "    os.makedirs(label_folder_name)\n",
    "\n",
    "best_model = bert_classifier\n",
    "best_model.load_state_dict(torch.load(f\"./final_github/best_model.pt\"))\n",
    "best_model.to(device)\n",
    "\n",
    "probs = bert_predict(best_model, test_dataloader)\n",
    "\n",
    "all_prob.extend(probs)\n",
    "\n",
    "val_preds = np.argmax(probs, axis=1)\n",
    "\n",
    "fold_metrics = {\n",
    "    'predicted_probs 0': [lst[0] for lst in probs],\n",
    "    'predicted_probs 1': [lst[1] for lst in probs],\n",
    "    'predicted_probs 2': [lst[2] for lst in probs],\n",
    "    'predicted_label': val_preds,\n",
    "    'true_label': y_test\n",
    "}\n",
    "\n",
    "fold_df = pd.DataFrame(fold_metrics, columns=['predicted_probs 0', 'predicted_probs 1',\n",
    "                                              'predicted_probs 2', 'predicted_label', 'true_label'])\n",
    "\n",
    "label_file_path = os.path.join(label_folder_name, f'./label.csv')\n",
    "fold_df.to_csv(label_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    prediction = y_pred\n",
    "    merged_list = y_true\n",
    "\n",
    "    report = classification_report(merged_list, prediction, digits=4)\n",
    "    matrix = confusion_matrix(merged_list, prediction)\n",
    "    f1_score_micro = metrics.f1_score(merged_list, prediction, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(merged_list, prediction, average='macro')\n",
    "    precision = precision_score(merged_list, prediction, average='macro')\n",
    "    recall = recall_score(merged_list, prediction, average='macro')\n",
    "    accuracy = accuracy_score(merged_list, prediction)\n",
    "\n",
    "    tn = matrix[0, 0]\n",
    "    tp = matrix[1, 1]\n",
    "    fp = matrix[0, 1]\n",
    "    fn = matrix[1, 0]\n",
    "    specificity = tn / (fp + tn)\n",
    "    NPV = tn / (fn + tn)\n",
    "    PPV = tp / (fp + tp)\n",
    "    \n",
    "    results = {\n",
    "        'report': report,\n",
    "        'confusion_matrix': matrix,\n",
    "        'f1_score_micro': f1_score_micro,\n",
    "        'f1_score_macro': f1_score_macro,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'specificity': specificity,\n",
    "        'NPV': NPV,\n",
    "        'PPV': PPV\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1_macro: {f1_score_macro:.4f}\")\n",
    "    print(f\"F1_micro: {f1_score_micro:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Sensitivity: {recall:.4f}\")\n",
    "    print(f\"PPV: {PPV:.4f}\")\n",
    "    print(f\"NPV: {NPV:.4f}\")\n",
    "    print(\"=============================\")\n",
    "    print(\"Classification Report: \\n\", report)\n",
    "    print(\"=============================\")\n",
    "    print(\"Confusion Matrix: \\n\", matrix)\n",
    "    \n",
    "    return results\n",
    "\n",
    "metrics_results = calculate_metrics(y_test, val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MODEL1_PREDICTION = pd.read_csv(\"YOUR_MODEL_PREDICTION1.csv\")\n",
    "MODEL2_PREDICTION = pd.read_csv(\"YOUR_MODEL_PREDICTION2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine models\n",
    "COMBINED_PREDICTION = pd.concat([MODEL1_PREDICTION, MODEL2_PREDICTION], axis=1)\n",
    "\n",
    "# COUNT AVERAGE SCORE\n",
    "VOTING_RESULT = COMBINED_PREDICTION.groupby(COMBINED_PREDICTION.columns, axis=1).mean()\n",
    "\n",
    "# FIND THE MAX LABEL\n",
    "VOTING_RESULT['pred_label'] = VOTING_RESULT.idxmax(axis=1)\n",
    "VOTING_RESULT = VOTING_RESULT.assign(max_value=VOTING_RESULT.max(axis=1))\n",
    "VOTING_RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env-pytorch2] *",
   "language": "python",
   "name": "conda-env-env-pytorch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
